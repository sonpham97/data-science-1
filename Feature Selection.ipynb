{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**30E03000 - Data Science for Business I (2021)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Feature Selection for Sparsity\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "This tutorial on Subset Selection, Ridge Regression, and Lasso is based on pp. 244-247 and pp. 251-255 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The Python code has been adapted from R. Jordan Crouser at Smith College.\n",
    "\n",
    "### Overview \n",
    "\n",
    "In this tutorial, you will get acquainted with the **bias-variance trade-off** problem in linear regression and how it can be solved with (sequential) feature selection algorithms and regularization.\n",
    "\n",
    "<img src=\"Images/variancebias.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The motivation behind feature selection algorithms is to automatically select a subset of features that is most relevant to the problem. The goal of feature selection is two-fold: We want to **improve the computational efficiency** and **reduce the generalization error** of the model by removing irrelevant features or noise.\n",
    "\n",
    "In this tutorial, we will discuss:\n",
    "- Best Subset Selection\n",
    "- Sequential Forward Selection (SFS)\n",
    "- Sequential Backward Selection (SBS)\n",
    "- Ridge Regression\n",
    "- Lasso\n",
    "\n",
    "### Problem setting\n",
    "\n",
    "In this tutorial, we are working with Baseball data that was collected over 2 seasons. We wish to **predict a baseball player’s salary** based on various statistics describing the performance in the previous year:\n",
    "\n",
    "- AtBat: Number of times at bat (season 1)\n",
    "- Hits: Number of hits (season 1)\n",
    "- HmRun: Number of home runs (season 1)\n",
    "- Runs: Number of runs (season 1)\n",
    "- RBI: Number of runs batted (season 1)\n",
    "- Walks: Number of walks (season 1)\n",
    "- Years: Number of years in the major leagues\n",
    "- CAtBat: Number of times at bat during his career\n",
    "- CHits: Number of hits during his career\n",
    "- CHmRun: Number of home runs during his career\n",
    "- CRuns: Number of runs during his career\n",
    "- CRBI: Number of runs batted in during his career\n",
    "- CWalks: Number of walks during his career\n",
    "- League: A factor with levels A and N indicating player's league (season 1)\n",
    "- Division: A factor with levels E and W indicating player's division (season 1)\n",
    "- PutOuts: Number of put outs (season 1)\n",
    "- Assists: Number of assists (season 1)\n",
    "- Errors: Number of errors (season 1)\n",
    "- Salary: 2nd season annual salary on opening day in thousands of dollars\n",
    "- NewLeague: A factor with levels A and N indicating player's league at the beginning of season 2\n",
    "\n",
    "**Source:** StatLib library at Carnegie Mellon University. \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/hit.jpeg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"Images/court.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "    <td> <img src=\"Images/ball.jpeg\" alt=\"Drawing\" style=\"width: 270px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >player</th>        <th class=\"col_heading level0 col1\" >AtBat</th>        <th class=\"col_heading level0 col2\" >Hits</th>        <th class=\"col_heading level0 col3\" >HmRun</th>        <th class=\"col_heading level0 col4\" >Runs</th>        <th class=\"col_heading level0 col5\" >RBI</th>        <th class=\"col_heading level0 col6\" >Walks</th>        <th class=\"col_heading level0 col7\" >Years</th>        <th class=\"col_heading level0 col8\" >CAtBat</th>        <th class=\"col_heading level0 col9\" >CHits</th>        <th class=\"col_heading level0 col10\" >CHmRun</th>        <th class=\"col_heading level0 col11\" >CRuns</th>        <th class=\"col_heading level0 col12\" >CRBI</th>        <th class=\"col_heading level0 col13\" >CWalks</th>        <th class=\"col_heading level0 col14\" >League</th>        <th class=\"col_heading level0 col15\" >Division</th>        <th class=\"col_heading level0 col16\" >PutOuts</th>        <th class=\"col_heading level0 col17\" >Assists</th>        <th class=\"col_heading level0 col18\" >Errors</th>        <th class=\"col_heading level0 col19\" >Salary</th>        <th class=\"col_heading level0 col20\" >NewLeague</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col0\" class=\"data row0 col0\" >-Andy Allanson</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col1\" class=\"data row0 col1\" >293</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col2\" class=\"data row0 col2\" >66</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col4\" class=\"data row0 col4\" >30</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col5\" class=\"data row0 col5\" >29</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col6\" class=\"data row0 col6\" >14</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col7\" class=\"data row0 col7\" >1</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col8\" class=\"data row0 col8\" >293</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col9\" class=\"data row0 col9\" >66</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col10\" class=\"data row0 col10\" >1</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col11\" class=\"data row0 col11\" >30</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col12\" class=\"data row0 col12\" >29</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col13\" class=\"data row0 col13\" >14</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col14\" class=\"data row0 col14\" >A</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col15\" class=\"data row0 col15\" >E</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col16\" class=\"data row0 col16\" >446</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col17\" class=\"data row0 col17\" >33</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col18\" class=\"data row0 col18\" >20</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col19\" class=\"data row0 col19\" >nan</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row0_col20\" class=\"data row0 col20\" >A</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col0\" class=\"data row1 col0\" >-Alan Ashby</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col1\" class=\"data row1 col1\" >315</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col2\" class=\"data row1 col2\" >81</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col3\" class=\"data row1 col3\" >7</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col4\" class=\"data row1 col4\" >24</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col5\" class=\"data row1 col5\" >38</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col6\" class=\"data row1 col6\" >39</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col7\" class=\"data row1 col7\" >14</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col8\" class=\"data row1 col8\" >3449</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col9\" class=\"data row1 col9\" >835</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col10\" class=\"data row1 col10\" >69</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col11\" class=\"data row1 col11\" >321</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col12\" class=\"data row1 col12\" >414</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col13\" class=\"data row1 col13\" >375</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col14\" class=\"data row1 col14\" >N</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col15\" class=\"data row1 col15\" >W</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col16\" class=\"data row1 col16\" >632</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col17\" class=\"data row1 col17\" >43</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col18\" class=\"data row1 col18\" >10</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col19\" class=\"data row1 col19\" >475</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row1_col20\" class=\"data row1 col20\" >N</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col0\" class=\"data row2 col0\" >-Alvin Davis</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col1\" class=\"data row2 col1\" >479</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col2\" class=\"data row2 col2\" >130</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col3\" class=\"data row2 col3\" >18</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col4\" class=\"data row2 col4\" >66</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col5\" class=\"data row2 col5\" >72</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col6\" class=\"data row2 col6\" >76</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col7\" class=\"data row2 col7\" >3</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col8\" class=\"data row2 col8\" >1624</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col9\" class=\"data row2 col9\" >457</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col10\" class=\"data row2 col10\" >63</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col11\" class=\"data row2 col11\" >224</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col12\" class=\"data row2 col12\" >266</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col13\" class=\"data row2 col13\" >263</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col14\" class=\"data row2 col14\" >A</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col15\" class=\"data row2 col15\" >W</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col16\" class=\"data row2 col16\" >880</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col17\" class=\"data row2 col17\" >82</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col18\" class=\"data row2 col18\" >14</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col19\" class=\"data row2 col19\" >480</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row2_col20\" class=\"data row2 col20\" >A</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col0\" class=\"data row3 col0\" >-Andre Dawson</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col1\" class=\"data row3 col1\" >496</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col2\" class=\"data row3 col2\" >141</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col3\" class=\"data row3 col3\" >20</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col4\" class=\"data row3 col4\" >65</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col5\" class=\"data row3 col5\" >78</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col6\" class=\"data row3 col6\" >37</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col7\" class=\"data row3 col7\" >11</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col8\" class=\"data row3 col8\" >5628</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col9\" class=\"data row3 col9\" >1575</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col10\" class=\"data row3 col10\" >225</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col11\" class=\"data row3 col11\" >828</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col12\" class=\"data row3 col12\" >838</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col13\" class=\"data row3 col13\" >354</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col14\" class=\"data row3 col14\" >N</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col15\" class=\"data row3 col15\" >E</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col16\" class=\"data row3 col16\" >200</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col17\" class=\"data row3 col17\" >11</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col18\" class=\"data row3 col18\" >3</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col19\" class=\"data row3 col19\" >500</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row3_col20\" class=\"data row3 col20\" >N</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col0\" class=\"data row4 col0\" >-Andres Galarraga</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col1\" class=\"data row4 col1\" >321</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col2\" class=\"data row4 col2\" >87</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col3\" class=\"data row4 col3\" >10</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col4\" class=\"data row4 col4\" >39</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col5\" class=\"data row4 col5\" >42</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col6\" class=\"data row4 col6\" >30</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col7\" class=\"data row4 col7\" >2</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col8\" class=\"data row4 col8\" >396</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col9\" class=\"data row4 col9\" >101</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col10\" class=\"data row4 col10\" >12</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col11\" class=\"data row4 col11\" >48</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col12\" class=\"data row4 col12\" >46</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col13\" class=\"data row4 col13\" >33</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col14\" class=\"data row4 col14\" >N</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col15\" class=\"data row4 col15\" >E</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col16\" class=\"data row4 col16\" >805</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col17\" class=\"data row4 col17\" >40</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col18\" class=\"data row4 col18\" >4</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col19\" class=\"data row4 col19\" >91.5</td>\n",
       "                        <td id=\"T_60372d54_75c5_11eb_a518_7bf5b0439d83row4_col20\" class=\"data row4 col20\" >N</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe5ec813cc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('hitters.csv')\n",
    "data.head().style #Call the head() function to see how the dataset looks (.style() shows all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of original data: (322, 21)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `data.info()`, we note that the `Salary` **variable is missing** for 59 players (263/322 entries non-null). Before doing any kind of feature selection, we have to remove the NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "As usual, we preprocess the data to allow for the subsequent modeling steps:\n",
    "- remove all entries where (at least) one variable is NaN, using Panda's `dropna()` function\n",
    "- perfrom one-hot-encoding for all string variables, using Panda's `get_dummies()` function\n",
    "- drop columns with no information value (player names) and columns that have been one-hot-encoded using `drop()`\n",
    "- merge the data with the one-hot-encoded dummy values, using `concat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 21)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop rows where (at least) one variable is NaN -> removes Salary = NaN values\n",
    "data = data.dropna() \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the player column (no predictive value)\n",
    "data = data.drop(['player'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Beware of multicollinearity when dealing with Regression models**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League_A</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_E</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_A</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   League_A  League_N  Division_E  Division_W  NewLeague_A  NewLeague_N\n",
       "1         0         1           0           1            0            1\n",
       "2         1         0           0           1            1            0\n",
       "3         0         1           1           0            0            1\n",
       "4         0         1           1           0            0            1\n",
       "5         1         0           0           1            1            0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use panda`s \"get_dummies()\" function to one-hot-encode the string variables into a new dataframe \n",
    "pd.get_dummies(data[['League', 'Division', 'NewLeague']]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the drop_first=True argument to fix the multicollinearity issue when performing one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['League', 'Division', 'NewLeague'], prefix=['League', 'Division', 'NewLeague'],\n",
    "                      drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >AtBat</th>        <th class=\"col_heading level0 col1\" >Hits</th>        <th class=\"col_heading level0 col2\" >HmRun</th>        <th class=\"col_heading level0 col3\" >Runs</th>        <th class=\"col_heading level0 col4\" >RBI</th>        <th class=\"col_heading level0 col5\" >Walks</th>        <th class=\"col_heading level0 col6\" >Years</th>        <th class=\"col_heading level0 col7\" >CAtBat</th>        <th class=\"col_heading level0 col8\" >CHits</th>        <th class=\"col_heading level0 col9\" >CHmRun</th>        <th class=\"col_heading level0 col10\" >CRuns</th>        <th class=\"col_heading level0 col11\" >CRBI</th>        <th class=\"col_heading level0 col12\" >CWalks</th>        <th class=\"col_heading level0 col13\" >PutOuts</th>        <th class=\"col_heading level0 col14\" >Assists</th>        <th class=\"col_heading level0 col15\" >Errors</th>        <th class=\"col_heading level0 col16\" >Salary</th>        <th class=\"col_heading level0 col17\" >League_N</th>        <th class=\"col_heading level0 col18\" >Division_W</th>        <th class=\"col_heading level0 col19\" >NewLeague_N</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col0\" class=\"data row0 col0\" >315</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col1\" class=\"data row0 col1\" >81</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col2\" class=\"data row0 col2\" >7</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col3\" class=\"data row0 col3\" >24</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col4\" class=\"data row0 col4\" >38</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col5\" class=\"data row0 col5\" >39</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col6\" class=\"data row0 col6\" >14</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col7\" class=\"data row0 col7\" >3449</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col8\" class=\"data row0 col8\" >835</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col9\" class=\"data row0 col9\" >69</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col10\" class=\"data row0 col10\" >321</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col11\" class=\"data row0 col11\" >414</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col12\" class=\"data row0 col12\" >375</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col13\" class=\"data row0 col13\" >632</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col14\" class=\"data row0 col14\" >43</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col15\" class=\"data row0 col15\" >10</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col16\" class=\"data row0 col16\" >475</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col17\" class=\"data row0 col17\" >1</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col18\" class=\"data row0 col18\" >1</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row0_col19\" class=\"data row0 col19\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col0\" class=\"data row1 col0\" >479</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col1\" class=\"data row1 col1\" >130</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col2\" class=\"data row1 col2\" >18</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col3\" class=\"data row1 col3\" >66</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col4\" class=\"data row1 col4\" >72</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col5\" class=\"data row1 col5\" >76</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col6\" class=\"data row1 col6\" >3</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col7\" class=\"data row1 col7\" >1624</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col8\" class=\"data row1 col8\" >457</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col9\" class=\"data row1 col9\" >63</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col10\" class=\"data row1 col10\" >224</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col11\" class=\"data row1 col11\" >266</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col12\" class=\"data row1 col12\" >263</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col13\" class=\"data row1 col13\" >880</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col14\" class=\"data row1 col14\" >82</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col15\" class=\"data row1 col15\" >14</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col16\" class=\"data row1 col16\" >480</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col17\" class=\"data row1 col17\" >0</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col18\" class=\"data row1 col18\" >1</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row1_col19\" class=\"data row1 col19\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col0\" class=\"data row2 col0\" >496</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col1\" class=\"data row2 col1\" >141</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col2\" class=\"data row2 col2\" >20</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col3\" class=\"data row2 col3\" >65</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col4\" class=\"data row2 col4\" >78</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col5\" class=\"data row2 col5\" >37</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col6\" class=\"data row2 col6\" >11</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col7\" class=\"data row2 col7\" >5628</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col8\" class=\"data row2 col8\" >1575</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col9\" class=\"data row2 col9\" >225</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col10\" class=\"data row2 col10\" >828</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col11\" class=\"data row2 col11\" >838</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col12\" class=\"data row2 col12\" >354</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col13\" class=\"data row2 col13\" >200</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col14\" class=\"data row2 col14\" >11</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col15\" class=\"data row2 col15\" >3</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col16\" class=\"data row2 col16\" >500</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col17\" class=\"data row2 col17\" >1</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col18\" class=\"data row2 col18\" >0</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row2_col19\" class=\"data row2 col19\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col0\" class=\"data row3 col0\" >321</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col1\" class=\"data row3 col1\" >87</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col2\" class=\"data row3 col2\" >10</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col3\" class=\"data row3 col3\" >39</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col4\" class=\"data row3 col4\" >42</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col5\" class=\"data row3 col5\" >30</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col6\" class=\"data row3 col6\" >2</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col7\" class=\"data row3 col7\" >396</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col8\" class=\"data row3 col8\" >101</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col9\" class=\"data row3 col9\" >12</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col10\" class=\"data row3 col10\" >48</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col11\" class=\"data row3 col11\" >46</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col12\" class=\"data row3 col12\" >33</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col13\" class=\"data row3 col13\" >805</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col14\" class=\"data row3 col14\" >40</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col15\" class=\"data row3 col15\" >4</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col16\" class=\"data row3 col16\" >91.5</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col17\" class=\"data row3 col17\" >1</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col18\" class=\"data row3 col18\" >0</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row3_col19\" class=\"data row3 col19\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col0\" class=\"data row4 col0\" >594</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col1\" class=\"data row4 col1\" >169</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col2\" class=\"data row4 col2\" >4</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col3\" class=\"data row4 col3\" >74</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col4\" class=\"data row4 col4\" >51</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col5\" class=\"data row4 col5\" >35</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col6\" class=\"data row4 col6\" >11</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col7\" class=\"data row4 col7\" >4408</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col8\" class=\"data row4 col8\" >1133</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col9\" class=\"data row4 col9\" >19</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col10\" class=\"data row4 col10\" >501</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col11\" class=\"data row4 col11\" >336</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col12\" class=\"data row4 col12\" >194</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col13\" class=\"data row4 col13\" >282</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col14\" class=\"data row4 col14\" >421</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col15\" class=\"data row4 col15\" >25</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col16\" class=\"data row4 col16\" >750</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col17\" class=\"data row4 col17\" >0</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col18\" class=\"data row4 col18\" >1</td>\n",
       "                        <td id=\"T_a29fc624_75c5_11eb_a518_7bf5b0439d83row4_col19\" class=\"data row4 col19\" >0</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe5ec7f5518>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of modified data: (263, 20)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a feature dataframe `X` and a target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Salary'].values #define target variable \n",
    "X = data.loc[:, data.columns != 'Salary'] #define feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "We split the data 70:30 into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do you need feature selection/sparse models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exog = sm.add_constant(X_train)\n",
    "result = sm.OLS(y_train, X_train).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTraining MSE: ' + str(round(mean_squared_error(y_train, result.predict(X_train)), 0)))\n",
    "print('Test MSE:    ' + str(round(mean_squared_error(y_test, result.predict(X_test)), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Problem: big performance difference between train and test!**</font>\n",
    "\n",
    "--> we want to find a sparse model (fewer features) in the hopes that such a model will generalize better to unseen test data.\n",
    "\n",
    "<img src=\"Images/variancebias_edit.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Best Subset Selection (the Brute Force approach)\n",
    "\n",
    "First, we will perfrom Best Subset Selection by identifying the best model that contains a given number of features, where **best** is quantified using the [Residual Sum of Squared (RSS)](https://en.wikipedia.org/wiki/Residual_sum_of_squares). We'll define a helper function `processSubset()` to outputs the best set of variables for each model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    \n",
    "    features = list(feature_set)\n",
    "    if 'const' not in features:\n",
    "        features.append('const')\n",
    "    \n",
    "    exog = X_train[features]\n",
    "    exog_test = X_test[features]\n",
    "\n",
    "    model = sm.OLS(y_train, exog)\n",
    "    regr = model.fit()\n",
    "    \n",
    "    train_MSE = round(mean_squared_error(y_train, regr.predict(exog)), 0)\n",
    "    test_MSE = round(mean_squared_error(y_test, regr.predict(exog_test)), 0)\n",
    "    \n",
    "    RSS = ((regr.predict(exog) - y_train) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS, \"Train MSE\": train_MSE, \"Test MSE\": test_MSE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "        \n",
    "    for combo in itertools.combinations(X.columns[1:-1], k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (round(toc-tic,3)), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we want to call that function for each number of features $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"model\", \"RSS\", \"Train MSE\", \"Test MSE\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,7): #between 1 and 7 features\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency matters: the Best Subset selection algorithm is computationally heavy. The CS Jupyter Hub is not able to handle more than 6 predictors! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one big Dataframe `models_best` that contains the best models for each $k$ value along with their RSS, Train MSE, and Test MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the details of each model by calling the `summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_best.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the best **two-variable model** contains `Hits` and `CRBI`. \n",
    "\n",
    "Printing the above summery for all 6 models would quickly clutter the notebook. Instead of calling the `summary()` function, we can direcly access the models' attributes we are interested in, e.g. adjusted $R^2$, AIC, and BIC. We can examine these to try to select the best overall model. Let's start by looking at $R^2$ across all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models_best.loc[2, \"model\"].rsquared\n",
    "\n",
    "# Gets the second element from each row ('model') and pulls out its rsquared attribute\n",
    "models_best.apply(lambda row: row[0].rsquared, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the $R^2$ statistic increases monotonically as more variables are included.\n",
    "\n",
    "Plotting MSE, adjusted $R^2$, AIC, and BIC for all of the models at once will help us decide which model to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureSelection(model):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "    # Set up a 2x2 grid so we can look at 4 plots at once\n",
    "    plt.subplot(2, 2, 1)\n",
    "\n",
    "    # We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "    # The idxmax() function can be used to identify the location of the maximum point of a vector\n",
    "    #plt.plot(model[\"RSS\"])\n",
    "    #plt.xticks(model.index)\n",
    "    #plt.xlabel('# Predictors')\n",
    "    #plt.ylabel('RSS')\n",
    "\n",
    "    plt.plot(model[\"Train MSE\"], label='train')\n",
    "    plt.plot(model[\"Test MSE\"], label='test')\n",
    "    plt.xticks(model.index)\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    # We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "    # The idxmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "    rsquared_adj = model.apply(lambda row: row[0].rsquared_adj, axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(rsquared_adj)\n",
    "    plt.plot(rsquared_adj.idxmax(), rsquared_adj.max(), \"or\")\n",
    "    plt.xticks(model.index)\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('adjusted rsquared')\n",
    "\n",
    "    # We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "    aic = model.apply(lambda row: row[0].aic, axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(aic)\n",
    "    plt.plot(aic.idxmin(), aic.min(), \"or\")\n",
    "    plt.xticks(model.index)\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('AIC')\n",
    "\n",
    "    bic = model.apply(lambda row: row[0].bic, axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(bic)\n",
    "    plt.plot(bic.idxmin(), bic.min(), \"or\")\n",
    "    plt.xticks(model.index)\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('BIC')\n",
    "\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeatureSelection(models_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that according to adjusted $R^2$ something, AIC, and BIC the 6-variable model is best. Again, no one measure is going to give us an entirely accurate picture... but they all agree that a model with 5 or fewer predictors is insufficient.\n",
    "\n",
    "In summary, the Best Subset Selection approach resembles a brute force approach in the way that it simply tries all possible combinations. It becomes computationally heavy very quickly, even on small datasets (which is why we did not explore beyond k = 7 in the above example). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Forward Selection\n",
    "\n",
    "A more elegant approach is Forward Selection (FWS). With a slight modification, The Best Subset Selection code can be used for Forward Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns[1:-1] if p not in predictors]\n",
    "    #print(remaining_predictors)\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "        #print(predictors+[p])\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (round(toc-tic,3)), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"model\", \"RSS\", \"Train MSE\", \"Test MSE\"])\n",
    "\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "predictors = ['const']\n",
    "#predictors.append(X.)\n",
    "\n",
    "for i in range(1,len(X.columns[1:-1])+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward-stepwise selection is a greedy algorithm, producing a nested sequence of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward selection approach runs sooooo much faster, even when evaluating models with large k values!\n",
    "\n",
    "<img src=\"Images/vegeta.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the Best Subset Selection, Forward Selection suggests that the best 2-variable model includes `Hits` and `CRBI` (not shown here). Additionally, we compare the best 6-variable model for each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_best.loc[6, \"model\"].summary())\n",
    "print()\n",
    "print(models_fwd.loc[6, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods suggest different variables. Moreover, Forward Selection found the subset **significantly** faster!\n",
    "\n",
    "Plotting MSE, adjusted $R^2$, AIC, and BIC for all of the models at once will help us decide which model to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeatureSelection(models_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware: once a variable is including in the model it will stay there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Backwards Selection\n",
    "\n",
    "We recycle the Forward Selection code, but loop through the predictors in reverse to perform Backwards Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-2):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (round(toc-tic,3)), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"model\", \"RSS\", \"Train MSE\", \"Test MSE\"])#, \n",
    "                          #index = range(1,len(X.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = X.columns[1:-1]\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_bwd.loc[6, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the best 6-variable models for Best Subset, Forwards, and Backwards Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, name in zip([models_best, models_fwd, models_bwd], ['Best Subset', 'Forward Selection', 'Backward Selection']):\n",
    "    print(\"\\n------------\")\n",
    "    print(name)\n",
    "    print(\"------------\")\n",
    "    print(model.loc[6, \"model\"].params.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting MSE, adjusted $R^2$, AIC, and BIC for all of the models at once will help us decide which model to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeatureSelection(models_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/variancebias_edit_3.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 4: Ridge Regression\n",
    "\n",
    "Ridge estimates are found by minimizing (from slides):\n",
    "\n",
    "<img src=\"Images/ridge.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\n",
    "- **The second term is a penalty that shrinks the coefficients towards zero**\n",
    "- Though not immediately obvious, shrinking can help to reduce variance\n",
    "- The estimates are very sensitive to the “tuning parameter” lambda -> needs to be found separately using cross validation!\n",
    "- The tuning parameter controls the relative impact of RSS and penalty on the regression coefficient estimates.\n",
    "\n",
    "We will use a `sklearn` package in order to perform ridge regression. Specifically, we will be using `Ridge()` to fit ridge regression models and its cross-validated counterpart `RidgeCV()`.\n",
    "\n",
    "\n",
    "The `Ridge()` function has an alpha argument **($\\lambda$, but with a different name!)** that is used to tune the model. We'll generate an array of alpha values ($\\lambda$!!!) ranging from very big to very small, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas)\n",
    "plt.ylabel('alpha value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with each alpha value is a vector of ridge regression coefficients, which we'll store in a matrix `coefs`. In this case, it is a $19 \\times 100$ matrix, with 19 rows (one for each predictor) and 100 columns (one for each value of alpha). Remember that we'll want to standardize the variables so that they are on the same scale. To do this, we can use the `normalize = True` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of $l_2$ norm, when a large value of alpha is used, as compared to when a small value of alpha is used. Let's plot and find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using $\\lambda = 4$ (the lambda value is **chosen randomly** for illustrative purposes. We'll cover how to pick an optimal value in a second):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2 = Ridge(alpha = 4, normalize = True).fit(X_train, y_train) # Fit a ridge regression on the training data\n",
    "pred2 = ridge2.predict(X_test) # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "\n",
    "print('\\nTraining MSE: ' + str(round(mean_squared_error(y_train, ridge2.predict(X_train)), 0)))\n",
    "print('\\nTest MSE:     ' + str(round(mean_squared_error(y_test, ridge2.predict(X_test)), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/variancebias_edit_4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE when alpha = 4 is 129,245. Now let's see what happens if we use a huge value of alpha, say $10^{10}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge3 = Ridge(alpha = 10**10, normalize = True)\n",
    "ridge3.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred3 = ridge3.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge3.coef_, index = X.columns)) # Print coefficients\n",
    "print('\\nTraining MSE: ' + str(round(mean_squared_error(y_train, ridge3.predict(X_train)), 0)))\n",
    "print('\\nTest MSE:     ' + str(round(mean_squared_error(y_test, ridge3.predict(X_test)), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This big penalty shrinks the coefficients to a very large degree, essentially reducing to a model containing just the intercept. This **over-shrinking makes the model more biased**, resulting in a higher MSE.\n",
    "\n",
    "<img src=\"Images/variancebias_edit_5.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Okay, so fitting a ridge regression model with alpha = 4 leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with alpha = 4 instead of just performing least squares regression. Recall that least squares is simply ridge regression with alpha = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge4 = Ridge(alpha = 0, normalize = True).fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred = ridge4.predict(X_test)            # Use this model to predict the test data\n",
    "print(pd.Series(ridge4.coef_, index = X.columns)) # Print coefficients\n",
    "print('\\nTraining MSE: ' + str(round(mean_squared_error(y_train, ridge4.predict(X_train)), 0)))\n",
    "print('\\nTest MSE:     ' + str(round(mean_squared_error(y_test, ridge4.predict(X_test)), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are indeed improving over regular least-squares!\n",
    "\n",
    "Instead of arbitrarily choosing alpha $ = 4$, it would be better to use cross-validation to choose the tuning parameter alpha. We can do this using the cross-validated ridge regression function, `RidgeCV()`. By default, the function performs generalized cross-validation (an efficient form of LOOCV), though this can be changed using the argument `cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we see that the value of alpha that results in the smallest cross-validation\n",
    "error is 0.57. What is the test MSE associated with this value of\n",
    "alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge5 = Ridge(alpha = ridgecv.alpha_, normalize = True).fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge5.predict(X_test))\n",
    "print('\\nTraining MSE: ' + str(round(mean_squared_error(y_train, ridge5.predict(X_train)), 0)))\n",
    "print('\\nTest MSE:    ' + str(round(mean_squared_error(y_test, ridge5.predict(X_test)), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike subset selection, which will generally select models that involve just a subset of the variables, \n",
    "<font color='red'>**ridge regression will include all p predictors in the final model!** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 5: LASSO\n",
    "\n",
    "Similar to ridge regression, but the key difference in behavior follows from penalty:\n",
    "\n",
    "<img src=\"Images/lasso.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We saw that ridge regression with a wise choice of alpha can outperform least\n",
    "squares as well as the null model on the Hitters data set. We now ask\n",
    "whether the lasso can yield either a more accurate or a more interpretable\n",
    "model than ridge regression. In order to fit a lasso model, we'll\n",
    "use the `Lasso()` function; however, this time we'll need to include the argument `max_iter = 10000`.\n",
    "Other than that change, we proceed just as we did in fitting a ridge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train.astype(float)), y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the coefficient plot that depending on the choice of tuning\n",
    "parameter, some of the coefficients are exactly equal to zero. We now\n",
    "perform 10-fold cross-validation to choose the best alpha, refit the model, and compute the associated test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "print('\\nTraining MSE: ' + str(round(mean_squared_error(y_train, lasso.predict(X_train)), 0)))\n",
    "print('\\nTest MSE:    ' + str(round(mean_squared_error(y_test, lasso.predict(X_test)), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is substantially lower than the test set MSE of the null model and of\n",
    "least squares, and only a little worse than the test MSE of ridge regression with alpha\n",
    "chosen by cross-validation.\n",
    "\n",
    "**However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse.** Here we see that 13 of\n",
    "the 19 coefficient estimates are exactly zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge vs. Lasso\n",
    "\n",
    "<img src=\"Images/ridgevslasso.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final throughts\n",
    "\n",
    "- dont use the forward and backwards selection code that was used in this tutorial! It is simple but inefficient!\n",
    "- Forward selection using mlxtend -> Sequential Feature Selector: https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
