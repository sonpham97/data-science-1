{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**30E03000 - Data Science for Business I (2021)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Shopping Baskets analysis and Association Rule mining with Apriori\n",
    "\n",
    "### Business problem\n",
    "\n",
    "Association rule mining is a technique to identify underlying relations between different items. Take an example of a supermarket where customers can buy a variety of items. Usually there is a pattern in what customers buy. For instance, parents buy baby products such as milk and diapers **together**. University students may buy beer and chips **together**. In short, **transactions involve patterns. More profit can be generated if the relationship between the items purchased in different transactions can be identified.** \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/kplussa.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"Images/skortti.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "If item A and B are bought together more frequently then several steps can be taken to increase the profit. For example:\n",
    "\n",
    "1. A and B can be placed together so that when a customer buys one of the product she doesn't have to go far away to buy the other product.\n",
    "2. People who buy one of the products can be targeted through an advertisement campaign to buy the other.\n",
    "3. Collective discounts can be offered on these products if the customer buys both of them.\n",
    "4. Both A and B can be packaged together. \n",
    "\n",
    "<font color='red'>**The process of identifying an associations between products is called association rule mining.**</font> [[1]](https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this tutorial we will learn:\n",
    "- how to perform association rule mining in Python with the **Apriori algorithm** to find common shopping patterns,\n",
    "- how to define new variables based on these patterns, \n",
    "- how to use background data to understand what kind of customers are behind these patterns, \n",
    "- how to apply a Decision Tree algorithm to connect background data to the newly found (\"mined\") shopping patterns.\n",
    "\n",
    "### Key words\n",
    "\n",
    "`Apriori`, `association rule mining`, `shopping basket analysis`, `Decision Trees`\n",
    "\n",
    "### Happy shopping!\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/blackFriday.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori theory recap\n",
    "\n",
    "There are three major components of the Apriori algorithm:\n",
    "\n",
    "- Support\n",
    "- Confidence\n",
    "- Lift\n",
    "\n",
    "Suppose we have a record of 1000 customer transactions, and we want to find the Support, Confidence, and Lift for two items e.g. burgers and ketchup. Out of 1000 transactions, 100 contain ketchup while 150 contain a burger. In 50 out of the 150 burger transactions, customers also bought ketchup. \n",
    "\n",
    "Using this data, we want to find the support, confidence, and lift.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/burgerKetchup.png\" width=\"400\">\n",
    "\n",
    "#### Support\n",
    "\n",
    "Support refers to the **default popularity of an item** and can be calculated by finding the number of transactions containing a particular item divided by total number of transactions. Suppose we want to find support for item B. This can be calculated as:\n",
    "\n",
    "`Support(B) = (Transactions containing (B))/(Total Transactions)`\n",
    "\n",
    "For instance if out of 1000 transactions, 100 transactions contain Ketchup then the support for item Ketchup can be calculated as:\n",
    "\n",
    "`Support(Ketchup) = (Transactions containingKetchup)/(Total Transactions)\n",
    "Support(Ketchup) = 100/1000 = 10%`\n",
    "\n",
    "#### Confidence\n",
    "\n",
    "Confidence refers to the **likelihood that an item B is also bought if item A is bought.** It can be calculated by finding the number of transactions where A and B are bought together, divided by total number of transactions where A is bought. Mathematically, it can be represented as:\n",
    "\n",
    "`Confidence(A→B) = (Transactions containing both (A and B))/(Transactions containing A)` \n",
    "\n",
    "Coming back to our problem, we had 50 transactions where Burger and Ketchup were bought together. While in 150 transactions, burgers are bought. Then we can find likelihood of buying ketchup when a burger is bought can be represented as confidence of Burger -> Ketchup and can be mathematically written as:\n",
    "\n",
    "`Confidence(Burger→Ketchup) = (Transactions containing both (Burger and Ketchup))/(Transactions containing Burger)\n",
    "Confidence(Burger→Ketchup) = 50/150 = 33.3%`\n",
    "\n",
    "#### Lift\n",
    "\n",
    "`Lift(A -> B)` refers to the **increase in the ratio of sale of B when A is sold.** Lift(A –> B) can be calculated by dividing `Confidence(A -> B)` by `Support(B)`. Mathematically it can be represented as:\n",
    "\n",
    "`Lift(A→B) = (Confidence (A→B))/(Support (B))`\n",
    "\n",
    "Coming back to our Burger and Ketchup problem, the `Lift(Burger -> Ketchup)` can be calculated as:\n",
    "\n",
    "`Lift(Burger→Ketchup) = (Confidence (Burger→Ketchup))/(Support (Ketchup))\n",
    "Lift(Burger→Ketchup) = 33.3/10 = 3.33`\n",
    "\n",
    "Lift basically tells us that the likelihood of buying a Burger and Ketchup together is 3.33 times more than the likelihood of just buying the ketchup. A Lift of 1 means there is no association between products A and B. Lift of greater than 1 means products A and B are more likely to be bought together. Finally, Lift of less than 1 refers to the case where two products are unlikely to be bought together.\n",
    "\n",
    "## Steps involved in Apriori\n",
    "\n",
    "For large sets of data, there can be hundreds of items in hundreds of thousands transactions. The Apriori algorithm tries to extract rules for each possible combination of items. For instance, Lift can be calculated for item 1 and item 2, item 1 and item 3, item 1 and item 4 and then item 2 and item 3, item 2 and item 4 and then combinations of items e.g. item 1, item 2 and item 3; similarly item 1, item 2, and item 4, and so on.\n",
    "\n",
    "As you can see from the above example, this process can be extremely slow due to the number of combinations. To speed up the process, we need to perform the following steps:\n",
    "\n",
    "1. **Set a minimum treshold for support**. This means that we are only interested in finding rules for the items that have certain default existence (e.g. support) and have a minimum value for co-occurrence with other items (e.g. confidence).\n",
    "2. Extract all the subsets having higher value of support than minimum threshold.\n",
    "3. Select all the rules from the subsets with confidence value higher than minimum threshold.\n",
    "4. Order the rules by descending order of Lift.\n",
    "\n",
    "Enough with the theory, let's get our hands dirty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "The first step, as always, is to import the required libraries. \n",
    "\n",
    "Luckily, we do not need to write the Apriori script to calculate support, confidence, and lift for all the possible combination of items. We will use an off-the-shelf library where all of the code has already been implemented: [mlxtend](https://github.com/rasbt/mlxtend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mlxtend #run this if the package has not been installed yet\n",
    "#pip install networkx #run this if the package has not been installed yet\n",
    "#config Completer.use_jedi = False #to fix a weird bug that prevents autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #data frames (for storing data)\n",
    "import numpy as np #scientific computing\n",
    "import itertools \n",
    "\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "\n",
    "#Apriori library\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "#Visualize association rules\n",
    "import networkx as nx  \n",
    "import random\n",
    "\n",
    "#sklearn for modeling\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree algorithm\n",
    "from sklearn.model_selection import train_test_split #Data split function\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Decision tree plotting\n",
    "import pydotplus\n",
    "from IPython.display import Image \n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('shoppingdata.txt')\n",
    "data.head() #Let's call the head() function to see how the dataset looks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA)\n",
    "\n",
    "Our data set has 18 features (dimensions) and 1000 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shopping items (fruitveg, freshmeat, ...) are encoded as \"F\" or \"T\", indicated whether a shopping basket includes the item. Calling the `data.info()` function, we observe that these variables are of type object. **Pandas reads them as strings, not as boolean (True, False).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm cannot make sense of the strings, so we have to transform them into boolean values.\n",
    "\n",
    "It is tempting to tell Pandas to replace *all* \"T\" and \"F\" values in the whole dataset with \"True\" and \"False\". This approach, however, would cause issues as it would also affect the gender column where \"female\" is encoded as \"F\". \n",
    "\n",
    "Instead, we store the column names of all shopping items in a list called `items` and then tell pandas to replace all values in these colums using the `replace`function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ['fruitveg', 'freshmeat', 'dairy',  'cannedveg', 'cannedmeat', 'frozenmeal', 'beer', \n",
    "         'wine', 'softdrink', 'fish', 'confectionery']\n",
    "data[items] = data[items].replace({'T':True, 'F':False}) #replace only values in the columns specified in \"items\"\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shopping items are now of type bool(en): True, False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Apriori: \"*what* is bought together?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the Apriori algorithm to find out which items are commonly sold together, so that the store owner can take action to place the related items together or advertise them together in order to have increased profit.\n",
    "\n",
    "First, we select all columns with shopping items (already stored in the `items` variable) and store them in a new dataframe `records`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = data[items]\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We select only items with a `min_support` of 10% in order to focus on the truly often bought items. (recall that a min support = 0.1 means that the inderlying item has been bought in at least 10% of all observations). \n",
    "\n",
    "\n",
    "2. We filter out itemsets that contain less than 2 items. The Apriori algorithm would otherwise list single items in the itemset (yes, this is annyoing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(records, min_support=0.1, use_colnames=True) #filter min_support=10%\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x)) #create length column\n",
    "frequent_itemsets[(frequent_itemsets['length'] >= 2)] #only list itemsets with at least 2 items \n",
    "#we do not remove the single items (length = 1) from the dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. With the just identified `frequent_itemsets` (what's bought together?), we call the `association_rules()` function from the Apriori library to find a set of rules. Note that we only include rules with a Lift of 1.6 or higher in order to focus on rules that are likely to increase sales (recall that Lift basically tells us that the likelihood of buying A and B together is x times higher than the likelihood of just buying B.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.6) #only include rules with Lift >= 1.6\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. This leaves us with 16 rules. Note how many rules are redundant (A -> B, B -> A).\n",
    "\n",
    "5. The tables above show **three unique association rules** or itemsets:\n",
    "    - wine and confectionary (chocolate)\n",
    "    - fish and fruit/vegetables\n",
    "    - beer, frozen vegetables and pizza\n",
    "    \n",
    "6. Plot the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = rules['support'].values\n",
    "confidence = rules['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below is arguably quite tricky. You are not expected to reproduce this in any assignment or exam! Either \n",
    "#it will be omitted or provided to you. It is included here to help you understand the connection of items and \n",
    "#rules by visualizing them in a \"spider web\".\n",
    "\n",
    "def draw_graph(rules, rules_to_show):\n",
    "    G1 = nx.DiGraph()\n",
    "    \n",
    "    color_map=[]\n",
    "    N = 50\n",
    "    colors = np.random.rand(N)\n",
    "    \n",
    "    strs = []\n",
    "    for i in range(len(rules)):\n",
    "        strs.append('R{}'.format(i))   \n",
    "   \n",
    "    for i in range (rules_to_show):      \n",
    "        G1.add_nodes_from([\"R\"+str(i)])\n",
    "        \n",
    "        for a in rules.iloc[i]['antecedents']:\n",
    "            G1.add_nodes_from([a])\n",
    "            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n",
    "       \n",
    "        for c in rules.iloc[i]['consequents']:\n",
    "            G1.add_nodes_from([c])\n",
    "            G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n",
    " \n",
    "    for node in G1:\n",
    "        found_a_string = False\n",
    "        for item in strs: \n",
    "            if node==item:\n",
    "                found_a_string = True\n",
    "        if found_a_string:\n",
    "            color_map.append('yellow')\n",
    "        else:\n",
    "            color_map.append('green')       \n",
    "   \n",
    "    edges = G1.edges()\n",
    "    colors = [G1[u][v]['color'] for u,v in edges]\n",
    "    weights = [G1[u][v]['weight'] for u,v in edges]\n",
    " \n",
    "    pos = nx.spring_layout(G1, k=16, scale=1)\n",
    "    plt.figure(1,figsize=(12,12)) \n",
    "\n",
    "    nx.draw(G1, pos, node_color = color_map, edge_color=colors, width=weights, font_size=16, with_labels=False)            \n",
    "   \n",
    "    for p in pos:  # raise text positions\n",
    "        pos[p][1] += 0.07\n",
    "    nx.draw_networkx_labels(G1, pos, font_size=16)\n",
    "    plt.show()\n",
    "\n",
    "draw_graph(rules, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"So, *who* buys it?\"\n",
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Make new variables</font> \n",
    "\n",
    "Based on these findings, **we create 3 new variables.** Each of them is True ONLY if all their subset variables are True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['wine_and_choco'] = ((records['wine']==True) & (records['confectionery']==True))\n",
    "data['fish_and_fruit'] = ((records['fish']==True) & (records['fruitveg']==True))\n",
    "data['beer_beans_pizza'] = ((records['beer']==True) & (records['cannedveg']==True) & (records['frozenmeal']==True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select feature variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe \"features\" that stores all explanatory/independent/feature variables that we will use to explain/predict whether customers will buy our newly found combined products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['value', 'pmethod', 'sex', 'homeown', 'income', 'age']\n",
    "X = data[features]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables\n",
    "\n",
    "Using the (by now familiar) pandas `get_dummies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=[\"pmethod\", \"sex\", \"homeown\"], prefix=[\"pmethod\", \"sex\", \"homeown\"]) #we add a prefix for easier identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Beer, Beans, Pizza\n",
    "\n",
    "### Data split\n",
    "\n",
    "\n",
    " <font color='red'>In previous tutorials, we had one \"global\" label/target vector `y` (e.g. fraudulent, reponse, etc.). Now, we have three different label vectors for the three newly-mined association rules. Thus, we have to specify a new target vector for each association rule and perform a split accordingly. Instead of just `y`, we call it `y_BBP` (for beer, beans, pizza):</font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_BBP = data['beer_beans_pizza']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_BBP, test_size = 0.3, random_state = 12345) #split data 70:30a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree model\n",
    "\n",
    "Train the Decision Tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and fit the Decision tree classifier with some default parameters\n",
    "clf = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=2, \n",
    "                             min_samples_leaf=3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use classifier to predict labels\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy is: \", (accuracy_score(y_test,y_pred)*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The graphviz library is used to visualize the tree. \n",
    "'''\n",
    "\n",
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=X_train.columns, \n",
    "                                class_names=['no buy', 'buy'], filled=True) #or use y_train.unique()\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())\n",
    "\n",
    "# Create PNG \n",
    "#graph.write_png(\"clf.png\") #uncomment this line to save the plot as a .png file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who buys beer, frozen vegetables and pizza? \n",
    "- male (propability of sex_M <= 0.5 = False (right branch) means male)\n",
    "- income below $ 16950 ( mean 20171, max 30000)\n",
    "\n",
    "Could be (university) students, low-income single men, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"Images/menpizza.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "<td> <img src=\"Images/pizzashirt.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td> <img src=\"Images/pizza.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "\n",
    "\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wine and chocolate\n",
    "\n",
    "### Data split\n",
    "\n",
    "And the same for wine and chocolate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_WC = data['wine_and_choco']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_WC, test_size = 0.3, random_state = 12345) #split data 70:30a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Decision tree classifier with some default parameters\n",
    "clf = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=3, \n",
    "                             min_samples_leaf=3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use classifier to predict labels\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy is: \", (accuracy_score(y_test,y_pred)*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The graphviz library is used to visualize the tree. \n",
    "'''\n",
    "\n",
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=X_train.columns, \n",
    "                                class_names=['no buy', 'buy'], filled=True) #or use y_train.unique()\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())\n",
    "\n",
    "# Create PNG \n",
    "#graph.write_png(\"clf.png\") #uncomment this line to save the plot as a .png file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who buys wine and chocolate?\n",
    "- female \n",
    "- income above 20050 ( mean 20171, max 30000)\n",
    "- shopping value > $32 \n",
    "\n",
    "Kinda hard to target beyond \"is female\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"Images/businesswine.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "<td> <img src=\"Images/winechoco.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/> </td\n",
    "\n",
    "\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fish & Fruit\n",
    "\n",
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_FF = data['fish_and_fruit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_FF, test_size = 0.3, random_state = 12345) #split data 70:30a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Decision tree classifier with some default parameters\n",
    "clf = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=2, \n",
    "                             min_samples_leaf=3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use classifier to predict labels\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy is: \", (accuracy_score(y_test,y_pred)*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The graphviz library is used to visualize the tree. \n",
    "'''\n",
    "\n",
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=X_train.columns, \n",
    "                                class_names=['no buy', 'buy'], filled=True) #or use y_train.unique()\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())\n",
    "\n",
    "# Create PNG \n",
    "#graph.write_png(\"clf.png\") #uncomment this line to save the plot as a .png file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who buys fish and fruit?\n",
    "- younger than 24.5 years\n",
    "- Do NOT own a home (probability of \"homeown_No\" < 0.5 = False is equivalent to prob. \"homeown_No\" > 0.5 = True) \n",
    "\n",
    "One possible explanation could be education (variable not available). High education might prevent early home ownership as students enter the worklife later and cannot afford a house in their mid-20s. In contrast, people who go for jobs that do not require higher education start earning money earlier and are able to afford their own place earlier. Yet, this young generation is more health conscious and opts for buying fish and fresh vegetables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"Images/uni.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "<td> <img src=\"Images/fish.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "    <td> <img src=\"Images/yoga.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "\n",
    "\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "In this tutorial we learned:\n",
    "- how to perform association rule mining in Python with the **Apriori algorithm** to find common shopping patterns,\n",
    "- how to define new variables based on these patterns, \n",
    "- how to use background data to understand what kind of customers are behind these patterns, \n",
    "- how to apply a Decision Tree algorithm to connect background data to the newly found (\"mined\") shopping patterns.\n",
    "\n",
    "With these skills, **we are able to understand not only \"what is frequently bought together\", but also WHO buys it.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
